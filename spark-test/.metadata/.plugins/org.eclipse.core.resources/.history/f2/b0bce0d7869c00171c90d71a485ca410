package tablon

import org.apache.spark._
import org.apache.spark.SparkContext._
import org.apache.log4j._

object test {
  
   def main(args: Array[String]) {
    
    val sc = new SparkContext("local[*]", "test")
    
    
     val c = sc.parallelize(Array(("est1", 1), ("est1", 1),("est1", 2)))
     c.reduceByKey(_+_).foreach(println) //(est1,4)
     c.map(x => (x._1+"-"+x._2,1)).reduceByKey(_+_).foreach(println)
     //(est1-2,1)
     //(est1-1,2)
     
     
     c.map(x => (x,x._1)).foreach(println) //Muestra todo el arreglo 
     //((est1,2),est1)
     //((est1,1),est1)
     //((est1,1),est1)
     
     println("Test")
     println(c.map(x => (x._2)).reduce(_+_)) //4 Suma todos los valores de y
     
    
     c.groupByKey().foreach(println) //(est1,CompactBuffer(1, 1, 2))
     
     
     val dataAlumnos = sc.parallelize(Array(("Martin", 20, 14), ("Iveth", 17,18),("Jorge", 15,18)))
     
     //dataAlumnos.map(x => (x._1,(x._2.toInt/x._3.toInt).toFloat)).foreach(println)
     dataAlumnos.map(x => (x._1,(x._2.toFloat+x._3.toFloat)/2.toFloat)).sortByKey().foreach(println) //Lo ordena por el nombre del alumno
     //(Iveth,17.5)
     //(Jorge,16.5)
     //(Martin,17.0)
   
     
     val dataBD = sc.parallelize(Array(("Sudamérica","Perú",30),("Sudamérica","Argentina",25),("Sudamérica","Colombia",20),("Europa","España",10)))
     println("Habitantes por países")
     dataBD.map(x => (x._1,x._3)).reduceByKey(_+_).foreach(println)
     
     
     
     
     
     
     
  }
  
}