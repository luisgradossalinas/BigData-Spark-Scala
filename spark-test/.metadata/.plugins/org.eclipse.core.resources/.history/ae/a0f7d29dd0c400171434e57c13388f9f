package TestDataFrame

import org.apache.spark._
import org.apache.spark.SparkContext._
import org.apache.log4j._
import java.util.ArrayList
import org.apache.spark.rdd.RDD
import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.types._
import org.apache.spark.sql._
import scala.collection.mutable.ArrayBuffer

object dframe {

  def main(args: Array[String]) {

    val sc = new SparkContext("local[*]", "dframe")
    val sqlContext = new org.apache.spark.sql.SQLContext(sc)

    
    val peopleRDD = sc.textFile("tablon.tsv")

    //Definir el nombre de los campos que tendrÃ¡ mi DataFrame
    val schemaString = "name age"

    // Generate the schema based on the string of schema
    val fields = schemaString.split(" ").map(fieldName => StructField(fieldName, StringType, nullable = true))
    val schema = StructType(fields)

    // Convert records of the RDD (people) to Rows
    val rowRDD = peopleRDD.map(_.split("\t")).map(attributes => Row(attributes(0), attributes(1).trim))

    // Apply the schema to the RDD
    val peopleDF = sqlContext.createDataFrame(rowRDD, schema)
    
    peopleDF.show()

    // Creates a temporary view using the DataFrame
    peopleDF.createOrReplaceTempView("tablon")

    // SQL can be run over a temporary view created using DataFrames
    val results = sqlContext.sql("SELECT * FROM tablon where age = 7767077")

    results.show()
    
    
    /*
    val dfs = sqlContext.read.json("employee.json")

    //sqlContext.createDataFrame(rdd, beanClass)

    dfs.show()
    dfs.printSchema()

    //Solo mostrar una columna
    dfs.select("name").show()
    //Filtrar el DataFrame
    dfs.filter(dfs("age") > 28).show()

    //Agrupados por edad
    //val x = dfs.groupBy("age").count()

    //dfs.groupBy("age").sum("age").show()

    //x.show()

    dfs.select(dfs("name"), dfs("age") + 1).show()
    * */
    

  }

}