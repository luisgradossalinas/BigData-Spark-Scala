package test

import org.apache.spark._
import org.apache.spark.rdd.RDD
import org.apache.spark.sql.types._
import org.apache.spark.sql._
import org.apache.spark.sql.functions._

object testing {

  val sc = new SparkContext("local[*]", "testing")
  val sqlContext = new org.apache.spark.sql.SQLContext(sc)

  def main(args: Array[String]) {

    val lista = List("100070934", "100070905")
    println("El primer valor de la lista es: " + lista(0))
    println("El segundo valor de la lista es: " + lista(1))

    val cabeceras = "Nombre Edad Num"
 
    val camposDF = cabeceras.split(" ").map(fieldName => StructField(fieldName, StringType, nullable = true))
    val schema = StructType(camposDF)
    
    val dataAlumnos = sc.parallelize(Array(("Martin", 20, 14), ("Iveth", 17, 18), ("Jorge", 15, 18)))
    
    sqlContext.createDataFrame(dataAlumnos, schema)
    
    println(lista.length)

  }

}