package TestDataFrame

import org.apache.spark._
import org.apache.spark.SparkContext._
import org.apache.log4j._
import org.apache.spark.rdd.RDD
import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.types._
import org.apache.spark.sql._
import scala.collection.mutable.ArrayBuffer

object SexoFrame {
  /**
   * Data del TablÃ³n
   * 17 => RUC
   * 6 => CODESTABLECIMIENTO
   * 12 => MTOTRANSACCION
   * 14 => CODCLAVECIC_CLIENTE
   * 25 => CODMES
   * 43 => SEXO_CLIENTE
   * 46 => RANGO_SUELDO
   * 47 => TIPUSODIGITAL
   * 48 => DESTIPUSODIGITAL
   * 57 => RANGO_EDAD
   */

  def byClientes(ruta: String, esta: List[String], inicio: String, fin: String): DataFrame = {

    val sc = new SparkContext("local[*]", "SexoFrame")
    val sqlContext = new org.apache.spark.sql.SQLContext(sc)

    val dataTablon = sc.textFile(ruta).map(r => r.split("\t")).map(r => Row(r(6), r(25), r(43)))
    val schemaString = "CODESTABLECIMIENTO CODMES SEXO_CLIENTE"
    val fields = schemaString.split(" ").map(fieldName => StructField(fieldName, StringType, nullable = true))
    val schema = StructType(fields)

    val tablonDF = sqlContext.createDataFrame(dataTablon, schema)

    return tablonDF.filter(tablonDF("CODESTABLECIMIENTO") isin ("100070934", "100070905")).filter(tablonDF("CODMES").between(inicio, fin)).groupBy("SEXO_CLIENTE").count()

  }

  def main(args: Array[String]) {

    val x = byClientes("tablon.tsv", List("100070934", "100070905"), "201501", "201512")
    x.show()

  }

}