package TestDataFrame

import org.apache.spark._
import org.apache.spark.SparkContext._
import org.apache.log4j._
import java.util.ArrayList
import org.apache.spark.rdd.RDD
import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.types._
//import spark.implicits._

object dframe {

  def main(args: Array[String]) {

    
    val sc = new SparkContext("local[*]", "prueba")
    val sqlContext = new org.apache.spark.sql.SQLContext(sc)
    
    
    val peopleRDD = sqlContext.sparkContext.textFile("tablon.tsv")
    
    

    /*
// The schema is encoded in a string
val schemaString = "name age"

// Generate the schema based on the string of schema
val fields = schemaString.split(" ")
  .map(fieldName => StructField(fieldName, StringType, nullable = true))
val schema = StructType(fields)

// Convert records of the RDD (people) to Rows
val rowRDD = peopleRDD
  .map(_.split(","))
  .map(attributes => Row(attributes(0), attributes(1).trim))

// Apply the schema to the RDD
val peopleDF = sqlContext.createDataFrame(rowRDD, schema)

*/
// Creates a temporary view using the DataFrame
peopleDF.createOrReplaceTempView("tablon")

// SQL can be run over a temporary view created using DataFrames
val results = spark.sql("SELECT name FROM people")
    
    
    
    
    
    val dfs = sqlContext.read.json("employee.json")
    
    //sqlContext.createDataFrame(rdd, beanClass)

    
    
    dfs.show()
    dfs.printSchema()
    
    //Solo mostrar una columna
    dfs.select("name").show()
    //Filtrar el DataFrame
    dfs.filter(dfs("age") > 28).show()
    
    //Agrupados por edad
    //val x = dfs.groupBy("age").count()
    
    //dfs.groupBy("age").sum("age").show()
    
    //x.show()
    
    dfs.select(dfs("name"), dfs("age") + 1).show()
    
    
    
  }

}