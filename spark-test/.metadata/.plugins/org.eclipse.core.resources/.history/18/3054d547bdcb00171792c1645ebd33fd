package Mis_Clientes

import org.apache.spark._
import org.apache.spark.rdd.RDD
import org.apache.spark.sql.types._
import org.apache.spark.sql._
import org.apache.spark.sql.functions._

object ProcedenciaClientes {
  
  val sc = new SparkContext("local[*]", "ProcedenciaClientes")
  
  val hiveContext = new org.apache.spark.sql.hive.HiveContext(sc)
  val tablonDF = hiveContext.sql("SELECT * FROM LONDON_SMART.TABLON where (codmes >= 201701 and codmes <= 201702) and rubro_bcp is not null")
 
  def KPI_RubroDepartamentoGeneralMesAnual(departamento: String, rubro: String, fecha: String): DataFrame = {

    return tablonDF.filter((tablonDF("DEPARTAMENTO_ESTABLEC").equalTo(departamento)) && (tablonDF("RUBRO_BCP").equalTo(rubro)) 
        && (!tablonDF("CODCLAVECIC_CLIENTE").equalTo("null"))
      && (tablonDF("CODMES").substr(1, 4).equalTo(fecha)))
      .groupBy("CODMES")
      .agg(
        countDistinct("CODCLAVECIC_CLIENTE").as("CANT_CLI_DIST"),
        count("RUBRO_BCP").as("CANT_TOT_TRX"),
        avg("MTOTRANSACCION").as("MONT_PROM_TRX"),
        sum("MTOTRANSACCION").as("MONT_TOT_TRX"),
        (sum("MTOTRANSACCION") / countDistinct("CODCLAVECIC_CLIENTE")).as("MONT_TRX_CLI"),
        (count("RUBRO_BCP") / countDistinct("CODCLAVECIC_CLIENTE")).as("PROM_TRX_CLI"))
      .orderBy("CODMES")

  }
  
  def main(args:Array[String]) {
    
    
    
  }
  
}